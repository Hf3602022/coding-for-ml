{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pandas1.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHeZeUSMKgrc"
      },
      "source": [
        "# Introduction to **`pandas`**\n",
        "\n",
        "`pandas` is a data science package orignally developed by [**Wes McKinney**](https://wesmckinney.com/).  It builds on top of `numpy` to provide a higher level API for wrangling, analysing and visualising data.  It is also closely coupled to `matplotlib` with a number of shorthand methods to create plots of data.\n",
        "\n",
        "In practice, I often use **both** `numpy` and `pandas` for **data wrangling**: switching between them (which is straightforward) when needed as each offers different benefits.\n",
        "\n",
        "For your Machine Learning and Data Science career you will find `pandas` very useful indeed.  It is very easy to import data from a variety of sources e.g. CSV or a database. (It also works with Excel, but I'd encourage you to not use Excel formatted files - save it as a CSV instead).\n",
        "\n",
        "**Read the full docs here: https://pandas.pydata.org/**\n",
        "\n",
        "> The higher level API offered by `pandas` comes at the cost of efficency i.e. execution speed.  **This statement is relative to `numpy`** which approaches the speed of `C`. I emphasise that `pandas` is still fast especially compared Microsoft Excel. However, in general `pandas` is not recommended for implementing high performance computational algorithms over and above `numpy`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOaFygxYOUbJ"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We usually use the alias `pd` when import `pandas`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snLfiEE1Ka2n"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_fpihquPdOm"
      },
      "source": [
        "print(pd.__version__)\n",
        "print(np.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpZ8jRErPr2R"
      },
      "source": [
        "## The `pd.Series` and `pd.DataFrame` classes\n",
        "\n",
        "The central contribution of the `pandas` package to data science in python are the `Series` and `DataFrame` class.  These provide the high level abstraction of data.\n",
        "\n",
        "### **`pd.Series`**\n",
        "\n",
        "If you are familiar with analysing tabular data in a software package like *stata*, *spss* or *Excel* then you can think of a series as a column of data.\n",
        "\n",
        "The example below:\n",
        "\n",
        "* Create a variable called `column1` of type `pd.Series`\n",
        "* `column1` has 10 rows (a sequence of 1 to 10).\n",
        "* The data in column 1 has a data type of 8-bit unsigned integer (0 - 255)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS4KksBcPf6h"
      },
      "source": [
        "# create a series\n",
        "column1 = pd.Series(np.arange(1, 10), name='unique_id', \n",
        "                    dtype=np.uint8)\n",
        "column1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcnUkfN936PG"
      },
      "source": [
        "# data type\n",
        "type(column1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oVY9q8p4zXZ"
      },
      "source": [
        "# shape\n",
        "column1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XorLb1hVSFpN"
      },
      "source": [
        "Another way to do this would have been to ignore the data type and name parameters.  Notice now that `pandas` has defaulted to `int64` for the data.  \n",
        "\n",
        "> Depending on your application, this may or may not be a big deal.  But note that it uses more memory. `int64` is equivalent to Python's `int` and C's `long` type: `-9_223_372_036_854_775_808` to `9_223_372_036_854_775_807`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwLGgjhfRPax"
      },
      "source": [
        "# create a series just passing data.\n",
        "column2 = pd.Series(np.arange(1, 10))\n",
        "column2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR5k884j4Em-"
      },
      "source": [
        "In a data science application its likely you will have a very large column of data.  You can take a look at the head or tail of the `Series` by using the `.head()` and `.tail()` methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sJq-RD4SRGS"
      },
      "source": [
        "# 10,000 rows. uint16 = 0 - 65_535\n",
        "column3 = pd.Series(np.arange(1, 10_000), name='longer', \n",
        "                    dtype=np.uint16)\n",
        "column3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPZ9b0EJ6I1N"
      },
      "source": [
        "# To view more or less rows\n",
        "column3.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4mVdRHj6ht1"
      },
      "source": [
        "column3.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qqe3kjzxY4_"
      },
      "source": [
        "# side note: to drop to numpy\n",
        "column3.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M8SkTM37cTN"
      },
      "source": [
        "###Â **`pd.DataFrame`**\n",
        "\n",
        "For most machine learning applications you will be working with a full `DataFrame`. \n",
        "\n",
        "The data you will use is likely to be imported from an external data source, such as a Comma Seperated Value (CSV) file or large scale database such as PostgreSQL.  But while you build familiarity with DataFrames we will look at building them manually.  \n",
        "\n",
        "> Building `DataFrames` manually is quite handy for practice, as I often use dataframes to summarise the results of computational procedures and simulations.  `DataFrames` can also generate LaTeX which is handy for quickly producing tables of results for a report/paper I am writing in LaTeX."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp8r66Ha8DMW"
      },
      "source": [
        "# create 5 x 4 matrix\n",
        "raw_data = np.arange(20, dtype=np.uint8).reshape(5, -1)\n",
        "raw_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMJb0tMv9_qj"
      },
      "source": [
        "raw_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ojjt43d6k4u"
      },
      "source": [
        "df = pd.DataFrame(raw_data)\n",
        "df.columns = (['col_' + str(i) for i in range(df.shape[1])])\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OdjMBw3_ZyL"
      },
      "source": [
        "# its a small matrix so lets view it all\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrrkBuLX_sr9"
      },
      "source": [
        "#for bigger `DataFrames` use .head()/.tail()\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuFZ1z8N9qfU"
      },
      "source": [
        "# access a named column\n",
        "df['col_3']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNyJ151eBZE3"
      },
      "source": [
        "# alternative approach for accessing\n",
        "df.col_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW8PZ3DA-jT-"
      },
      "source": [
        "# side bar 1 - let's ignore the datatype. \n",
        "# the size in memory is almost double.\n",
        "raw_data = np.arange(20).reshape(5, -1)\n",
        "df = pd.DataFrame(raw_data)\n",
        "df.columns = (['col_' + str(i) for i in range(df.shape[1])])\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzM3dyguxnX8"
      },
      "source": [
        "# side bar 2: drop to numpy\n",
        "df.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYcsmwpqh4La"
      },
      "source": [
        "### Creating a `pd.DataFrame` from python lists\n",
        "\n",
        "In the previous example we created a `DataFrame` from a `numpy.ndarray`.  But a `DataFrame` or `Series`can be created from anything *array-like*.  So for example, we could work with one or more python lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAAWswnbhwSL"
      },
      "source": [
        "# creating an individual series\n",
        "thrash_metal_bands = pd.Series(['pantera', 'metallica', 'megadeth',\n",
        "                                'anthrax'])\n",
        "thrash_metal_bands"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbXo2d9rjFt6"
      },
      "source": [
        "# create a full data frame\n",
        "\n",
        "# each column is defined as a seperate list\n",
        "band_name = ['pantera', 'metallica', 'megadeth', 'anthrax']\n",
        "n_albums = [9, 10, 15, 11]\n",
        "formed = [1981, 1981, 1983, 1981]\n",
        "still_active = [0, 1, 1, 1]\n",
        "\n",
        "# empty dataframe\n",
        "thrash_metal_bands = pd.DataFrame()\n",
        "\n",
        "# create new columns from lists\n",
        "thrash_metal_bands['band'] = band_name\n",
        "thrash_metal_bands['n_albums'] = n_albums\n",
        "thrash_metal_bands['yr_formed'] = formed\n",
        "thrash_metal_bands['active'] = still_active\n",
        "\n",
        "thrash_metal_bands"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENA4LqtDk-QK"
      },
      "source": [
        "# take a look at the df summary information.\n",
        "thrash_metal_bands.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl85g3iLkwCe"
      },
      "source": [
        "# could also be specific about datatype using pd.Series \n",
        "thrash_metal_bands = pd.DataFrame()\n",
        "\n",
        "thrash_metal_bands['band'] = pd.Series(band_name, dtype=str)\n",
        "thrash_metal_bands['n_albums'] = pd.Series(n_albums, dtype=np.uint8)\n",
        "thrash_metal_bands['yr_formed'] = pd.Series(formed, dtype=np.uint16)\n",
        "thrash_metal_bands['active'] = pd.Series(still_active, dtype=bool)\n",
        "\n",
        "thrash_metal_bands.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGkap58al3C0"
      },
      "source": [
        "Note that in practice its also useful to know that you can create a `DataFrame` from a dict.  I often forget exactly what format my data need to be in, but luckily you can call the `.to_dict()` method of a `DataFrame` to see what is required. \n",
        "\n",
        "> This functionality has proved useful in practice as it is often useful to use a simple `dict` to track results of an algorithm.  At the end of the experiment the dict can quickly be cast to a `DataFrame` and easily viewed in a notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N4lXqsnlvho"
      },
      "source": [
        "bands_dict = thrash_metal_bands.to_dict()\n",
        "bands_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctKkwoGSmW1J"
      },
      "source": [
        "# the code to use a dict looks like pandas code.\n",
        "bands_dict['band']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNL9YzFzmcFF"
      },
      "source": [
        "# reverse engineer\n",
        "new_df = pd.DataFrame(bands_dict)\n",
        "new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-dqPYNAnXly"
      },
      "source": [
        "## `DataFrame` Indexes\n",
        "\n",
        "In each of the examples so far you will notice that the first (unlabelled) a column is a sequence of arbitrary numbers.  This is the `DataFrame` index.  When we create a `DataFrame` manually an additional step is often to set the index to a column we have created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_9qxYtbA_tK"
      },
      "source": [
        "thrash_metal_bands.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAX7MILtnfix"
      },
      "source": [
        "# set the index to 'band' column\n",
        "# note that this method returns a 'copy' unless we set 'inplace=True'\n",
        "thrash_metal_bands.set_index('band', inplace=True)\n",
        "thrash_metal_bands"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouHVuCStoxLR"
      },
      "source": [
        "## Accessing elements in a `DataFrame`\n",
        "\n",
        "In nearly all data wrangling and analysis projects, you will want to explore subsets of your data.  This might be, for example, to eyeball the data close up or to calculate summary statistics for particular populations.  To do that in `pandas` we need to understand how we access individual and subsets or rows and columns in our `DataFrame`.  \n",
        "\n",
        "For simplicity, we will again use `thrash_metal_bands`.\n",
        "\n",
        "### Accessing Rows\n",
        "\n",
        "To access an individal row we can use the `.loc` and `.iloc` accessors.  The former of these references by **name** while the latter references by **numeric index**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMW1CglKowUz"
      },
      "source": [
        "# return the records for panteria\n",
        "thrash_metal_bands.loc['pantera']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le1FlTP2o1X9"
      },
      "source": [
        "# return the records the row at index 2\n",
        "thrash_metal_bands.iloc[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIkDMjcKqIRE"
      },
      "source": [
        "Note that we can return multiple rows if we supply a list of indexes.  For example to find records for both pantera and megadeth: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS5G4QXeqHzb"
      },
      "source": [
        "to_find = ['pantera', 'megadeth']\n",
        "thrash_metal_bands.loc[to_find]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QjLTKfhrYn6"
      },
      "source": [
        "Indexes can be sliced in a similar fashion to arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2W23vFrqgyv"
      },
      "source": [
        "# All rows from index 2 onwards\n",
        "thrash_metal_bands.iloc[2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMxNp6SQrl5W"
      },
      "source": [
        "# Rows 0 and 1\n",
        "thrash_metal_bands.iloc[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "036m8y19rq1J"
      },
      "source": [
        "# slicing by name\n",
        "thrash_metal_bands.loc['pantera': 'megadeth']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5V5siSRtk6t"
      },
      "source": [
        "### Accessing columns\n",
        "\n",
        "We have already seen that accessing a column is done as so \n",
        "\n",
        "```python\n",
        "df['column_name']\n",
        "```\n",
        "\n",
        "We can also select a multiple columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwlDDP4bso0B"
      },
      "source": [
        "# select yr_formed and active columns only\n",
        "mask = ['yr_formed', 'active']\n",
        "thrash_metal_bands[mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc576QX7vnPk"
      },
      "source": [
        "or restrict both columns and rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqMuTgKhtdh0"
      },
      "source": [
        "columns = ['yr_formed', 'active']\n",
        "rows = ['pantera', 'anthrax']\n",
        "thrash_metal_bands.loc[rows, mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdWzkquswhEC"
      },
      "source": [
        "### Selecting individual cells\n",
        "\n",
        "To access an individual cell within a `DataFrame` use the `.at` and `.iat` accessors.  The former uses the index row and column names while the latter is an integer based lookup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkssrLVuvh-m"
      },
      "source": [
        "# lookup the year pantera was formed.\n",
        "thrash_metal_bands.at['pantera', 'yr_formed']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MerHXuEGwqP8"
      },
      "source": [
        "# look up the cell value in position (1, 1)\n",
        "thrash_metal_bands.iat[1, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epgb9D1Xw6Di"
      },
      "source": [
        "# to update the individual value\n",
        "# lookup the year pantera was formed.\n",
        "thrash_metal_bands.at['pantera', 'yr_formed'] = 9999\n",
        "thrash_metal_bands"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ba-LKBLm7WK"
      },
      "source": [
        "# Reading data into pandas from a flat file\n",
        "\n",
        "A common task is to work with data stored in one or more files. For example in a Comma Seperated Value (CSV) or other type of delimited (e.g. tab or pipe) file.  \n",
        "\n",
        "There are a number of scenarios you may encounter:\n",
        "\n",
        "1. The data file is held locally on your machine (or network drive)\n",
        "2. The data file is accessed via a URL (e.g. it is located in GitHub or hosted on a third party website.)\n",
        "3. The data file is compressed e.g. in a `.zip` format\n",
        "\n",
        "> The good news is that reading from a local directory and reading from a remote URL is identical in `pandas`.  In both cases we can use the `pd.read_csv()` function specifying either the local path to the file or the url.\n",
        "\n",
        "As an example let's read in the famous [Wisonsin Breast Cancer dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) from Github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6uixF7tm-ky"
      },
      "source": [
        "# Wisconsin breast cancer dataset URL\n",
        "url = 'https://raw.githubusercontent.com/health-data-science-OR/' \\\n",
        "      + 'hpdm139-datasets/main/wisconsin.csv'\n",
        "\n",
        "# read into dataframe\n",
        "df = pd.read_csv(url, index_col='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnzYMXSMnHMO"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khtsnLx_AJNy"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiicRa5Uatls"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4iUZqSAH_s0"
      },
      "source": [
        "Now let's read in the same file, but this time it is compressed in .zip format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPf_3qYxu96W"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/health-data-science-OR/' \\\n",
        "      + 'hpdm139-datasets/main/wisconsin.zip'\n",
        "df = pd.read_csv(url, index_col='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0E7RJHBvieO"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j17__x7LIi_u"
      },
      "source": [
        "## End"
      ]
    }
  ]
}